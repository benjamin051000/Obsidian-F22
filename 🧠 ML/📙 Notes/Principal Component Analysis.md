AKA PCA

- Find a line that, when data in a space is projected onto, has the largest variance

-  Linear dimensionality reduction algorithm
- Linear because it's looking for the best hyperplane of projection
- Completely unsupervised

- Features generated by PCA do not carry physical meaning
	- Just linear combinations of original features
- New features explain the **most variance**

- Finds the rotation or projection using $Y=RX$ where $X, Y, R$ are $n\times2$ matricies
	- Does this via [[Eigenvectors and Eigenvalues]]

## Pseudo code
1. Subtract sample mean to the data, $x=x-\mu$ 
2. Compute covariance matrix of 1., called K, where K is DxD dimensions
3. Compute eigenvalues/vectors of K
4. To perform dimensionality reduction, pick top M eigenvalues

Typical threshold for PCA: 90% variance explainability
- Just add features until you meet the threshold
- Reduces a ton of dimensions
- 